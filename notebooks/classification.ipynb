{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=\"mps\" if torch.backends.mps.is_built() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    epoch = 20\n",
    "    seed = 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dir = \"/Users/yhemmy/Documents/code/hotel-id-experiments/dataset/randomHotels/randomHotelsFeats.csv\"\n",
    "df_pikcle_dir = \"/Users/yhemmy/Documents/code/hotel-id-experiments/dataset/randomHotels/randomHotelsFeats.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(df_dir,converters={\"hsv_feats\":pd.eval,\"rgb_feats\":pd.eval,\"hist_feats\":pd.eval})\n",
    "df = pd.read_pickle(df_pikcle_dir)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({\"hotel_id\":\"str\"})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]==df.image_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class\n",
    "### Moved to a script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.shape\n",
    "hotel_image_count = df.groupby(\"hotel_id\")[\"image_id\"].count()\n",
    "validation_hotels =  hotel_image_count[hotel_image_count>1]\n",
    "validation_hotels\n",
    "# validation_hotels.index\n",
    "# df[\"hotel_id\"].isin(validation_hotels.index)\n",
    "validation_data = df[df[\"hotel_id\"].isin(validation_hotels.index)]\n",
    "validation_df = validation_data.groupby(\"hotel_id\").sample(1,random_state=2024)\n",
    "# validation_df.shape\n",
    "train_df = df[~df[\"image_id\"].isin(validation_df[\"image_id\"])]\n",
    "# train_df.shape[0]+validation_df.shape[0]\n",
    "print(f\"Train data sample: {train_df.shape[0]} \\nValidation data sample: {validation_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = validation_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_dir = []\n",
    "for ind,row in train_df.iterrows():\n",
    "    image_id = row[\"image_id\"]\n",
    "    hotel_id = row[\"hotel_id\"]\n",
    "    path = row[\"path\"]\n",
    "\n",
    "    train_imgs_dir.append((path,hotel_id,image_id))\n",
    "\n",
    "validation_imgs_dir = []\n",
    "for ind,row in validation_df.iterrows():\n",
    "    image_id = row[\"image_id\"]\n",
    "    hotel_id = row[\"hotel_id\"]\n",
    "    path = row[\"path\"]\n",
    "\n",
    "    validation_imgs_dir.append((path,hotel_id,image_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = cv.imread(train_imgs_dir[500][0])\n",
    "sample = cv.resize(sample,(224,224))\n",
    "print(sample.shape)\n",
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = cv.imread(validation_imgs_dir[500][0])\n",
    "sample = cv.resize(sample,(224,224))\n",
    "print(sample.shape)\n",
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = df[\"hotel_id\"].unique()\n",
    "len(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.hotelsDataLoader import HOTELS\n",
    "    \n",
    "train_dataset = HOTELS(train_imgs_dir,unique_labels)\n",
    "validation_dataset = HOTELS(validation_imgs_dir,unique_labels)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    num_workers =1,\n",
    "    batch_size = 32,\n",
    "    shuffle = True\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset,\n",
    "    num_workers =1,\n",
    "    batch_size = 32,\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "# x, y,img_id = next(iter(train_dataloader))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_dataset.paths\n",
    "# validation_imgs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(x, y,img_id) in enumerate(validation_dataloader):\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(img_id)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test onehot encoding\n",
    "# t = train_imgs_dir[12000][1]\n",
    "# list(train_dataset.get_one_hot_encoding(t)).index(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"A batch holds {len(x)},{len(y)},{len(img_id)} of images, label and id respectively\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape,x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features size\n",
    "rgb_size = len(df[\"rgb_feats\"][0])\n",
    "hsv_size = len(df[\"hsv_feats\"][0])\n",
    "hist_size = len(df[\"hist_feats\"][0])\n",
    "\n",
    "num_classes=len(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, num_classes,features_dataframe,rgb_size,hsv_size,hist_size,embedding_size=128,\n",
    "                 backbone_name=\"efficientnet_b0\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features_dataframe = features_dataframe \n",
    "        # self.rgb_size = rgb_size\n",
    "        # self.hsv_size = hsv_size\n",
    "        # self.hist_size = hist_size   \n",
    "\n",
    "        self.num_classes = num_classes \n",
    "        self.backbone = timm.create_model(model_name = backbone_name,num_classes=num_classes, pretrained = True)\n",
    "        in_features = self.backbone.get_classifier().in_features\n",
    "\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        self.embedding = nn.Linear(in_features, embedding_size)\n",
    "        self.classifier = nn.Linear(embedding_size,num_classes)\n",
    "        self.rgbClassifier = nn.Linear(rgb_size+embedding_size,num_classes)\n",
    "        self.hsvClassifier = nn.Linear(hsv_size+embedding_size,num_classes)\n",
    "        self.histClassifier = nn.Linear(hist_size+embedding_size,num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "            \"\"\"\n",
    "            Return embeddings\n",
    "            \"\"\"\n",
    "            x = self.backbone(x)\n",
    "            x = x.view(x.size(0),-1)\n",
    "            x = self.embedding(x)\n",
    "            return x\n",
    "    \n",
    "    def extractColorFeatures(self,image_ids,feat=\"rgb_feats\"):\n",
    "        \"\"\"\n",
    "        return color features\n",
    "        \"\"\"\n",
    "        color_feature = []\n",
    "        for img_id in image_ids:\n",
    "            color_feats = df[df.image_id==img_id][feat].values[0]\n",
    "            color_feature.append(color_feats)\n",
    "        return color_feature\n",
    "\n",
    "    def fuseFeatures(self,features_embedding,features_color):\n",
    "        \"\"\"\n",
    "        return fused features i.e. embedding + color_features\n",
    "        \"\"\"\n",
    "        fused_features = []\n",
    "        for i,colorFeats in enumerate(features_color):\n",
    "            colorFeats =torch.tensor(colorFeats,dtype=torch.float).to(DEVICE)\n",
    "            embedding = features_embedding[i]\n",
    "            features = torch.cat((embedding,colorFeats))\n",
    "            fused_features.append(features)\n",
    "        # fused_features = torch.stack(fused_features,0)\n",
    "        return torch.stack(fused_features)\n",
    "\n",
    "    def classifyWithEmbedding(self,x):\n",
    "        \"\"\"\n",
    "        return hotel class using just embeddings\n",
    "        \"\"\"\n",
    "        hotel_class = self.classifier(x)\n",
    "        return hotel_class\n",
    "\n",
    "    def classifyWithFusedFeatures(self,fused_features,classifer_to_use):\n",
    "        \"\"\"\n",
    "        return hotel class using improved embeddings \n",
    "        \"\"\"\n",
    "        if classifer_to_use==\"rgb\":\n",
    "            hotel_class = self.rgbClassifier(fused_features)\n",
    "            return hotel_class\n",
    "        elif classifer_to_use==\"hsv\":\n",
    "            hotel_class = self.hsvClassifier(fused_features)\n",
    "            return hotel_class\n",
    "        else:\n",
    "            hotel_class = self.histClassifier(fused_features)\n",
    "            return hotel_class\n",
    "         \n",
    "\n",
    "    \n",
    "model = EmbeddingModel(num_classes,df,rgb_size,hsv_size,hist_size).to(DEVICE)\n",
    "# emb_ffff = model(torch.zeros((1, 3, 224, 224)).to(DEVICE))\n",
    "# print(emb_ffff.shape)\n",
    "# test_fussion = emb_ffff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model helper Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFeatures(dataloader,model,improveEmbedding = False,colorFeat= None):\n",
    "    features_all= []\n",
    "    target_all=[]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        bar_description = \"Generating embedding...\"\n",
    "        if(improveEmbedding):\n",
    "             bar_description = \"Extracting & improving embedding with {colorFeat}...\"\n",
    "\n",
    "        dataloader = tqdm(dataloader,desc=bar_description)\n",
    "        for batch_no,(x, y,img_ids) in enumerate(dataloader):\n",
    "                x = x.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "                x = model(x)\n",
    "                if(colorFeat):\n",
    "                    color_feats = model.extractColorFeatures(img_ids,colorFeat)\n",
    "                    x = model.fuseFeatures(x,color_feats)\n",
    "                    target_all.extend(y.cpu().numpy())\n",
    "                    features_all.extend(x.detach().cpu().numpy())\n",
    "                else:\n",
    "                    target_all.extend(y.cpu().numpy())\n",
    "                    features_all.extend(x.detach().cpu().numpy())\n",
    "                break\n",
    "    target_all = np.array(target_all).astype(np.float32)\n",
    "    features_all = np.array(features_all).astype(np.float32)\n",
    "    return features_all,target_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER =\"/Users/yhemmy/Documents/code/hotel-id-experiments/models/\"\n",
    "def save_checkpoint(model, scheduler, optimizer, epoch, name, loss=None, score=None):\n",
    "    checkpoint = {\"epoch\": epoch,\n",
    "                  \"model\": model.state_dict(),\n",
    "                  \"scheduler\": scheduler.state_dict(),\n",
    "                  \"optimizer\": optimizer.state_dict(),\n",
    "                  \"loss\": loss,\n",
    "                  \"score\": score,\n",
    "                  }\n",
    "\n",
    "    torch.save(checkpoint, f\"{OUTPUT_FOLDER}checkpoint-{name}.pt\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, scheduler, optimizer, name):\n",
    "    checkpoint = torch.load(f\"{OUTPUT_FOLDER}checkpoint-{name}.pt\")\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "    return model, scheduler, optimizer, checkpoint[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_one_hot(y_one_hot):\n",
    "    y = np.argmax(y_one_hot.cpu().numpy(), axis=1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_one_hot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MATCHES = 5\n",
    "def test_classification(loader, model,colorFeat= None):\n",
    "    targets_all = []\n",
    "    outputs_all = []\n",
    "    outputs = None\n",
    "    \n",
    "    model.eval()\n",
    "    dataloader = tqdm(loader, desc=\"Classification\")\n",
    "    \n",
    "    for batch_no,(x, y,img_ids) in enumerate(dataloader):\n",
    "        x = x.to(DEVICE)\n",
    "        y = decode_one_hot(y)\n",
    "        x = model(x)\n",
    "        #improve embedding\n",
    "        if(colorFeat and colorFeat==\"rgb_feats\"):\n",
    "            color_feats = model.extractColorFeatures(img_ids,colorFeat)\n",
    "            x = model.fuseFeatures(x,color_feats)\n",
    "            outputs = model.rgbClassifier(x)\n",
    "        elif(colorFeat and colorFeat==\"hsv_feats\"):\n",
    "            color_feats = model.extractColorFeatures(img_ids,colorFeat)\n",
    "            x = model.fuseFeatures(x,color_feats)\n",
    "            outputs = model.hsvClassifier(x)\n",
    "\n",
    "        elif(colorFeat and colorFeat==\"hist_feats\"):\n",
    "            color_feats = model.extractColorFeatures(img_ids,colorFeat)\n",
    "            x = model.fuseFeatures(x,color_feats)\n",
    "            outputs = model.histClassifier(x)\n",
    "        #use only embedding\n",
    "        else:\n",
    "            outputs = model.classifier(x)\n",
    "        targets_all.extend(y)\n",
    "        outputs_all.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "            \n",
    "        \n",
    "    \n",
    "    # repeat targets to N_MATCHES for easy calculation of MAP@5\n",
    "    y = np.repeat([targets_all], repeats=N_MATCHES, axis=0).T\n",
    "    # sort predictions in ascending order i.e least class to top class\n",
    "    sorted_indices = np.array(np.argsort(np.array(outputs_all),axis=1))\n",
    "    # flip to sort in descending order and get top 5 classes i.e top class to least class \n",
    "    preds = np.flip(sorted_indices,1)[:,:N_MATCHES]\n",
    "    preds = np.argsort(-np.array(outputs_all), axis=1)[:, :N_MATCHES]\n",
    "    # check if any of top 5 predictions are correct and calculate mean accuracy\n",
    "    acc_top_5 = (preds == y).any(axis=1).mean()\n",
    "    # calculate prediction accuracy\n",
    "    acc_top_1 = np.mean(targets_all == np.argmax(outputs_all, axis=1))\n",
    "\n",
    "    print(f\"Classification accuracy: {acc_top_1:0.4f}, MAP@5: {acc_top_5:0.4f}\")\n",
    "    return acc_top_1, acc_top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.repeat([[2,1]],2,0).T\n",
    "target\n",
    "# np.argsort(-np.array([1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.sigmoid(torch.tensor([[0.5,0.2,0.3],[0.4,0.4,0.2]]))\n",
    "sorted_indices = np.argsort(np.array(torch.sigmoid(torch.tensor([[0.5,0.2,0.3],[0.4,0.4,0.2]]))),-1)\n",
    "sorted_indices\n",
    "pred = np.flip(sorted_indices,1)[:,:2]\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred == target).any(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_all,t_all =generateFeatures(train_dataloader,model,improveEmbedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function with Color+Embedding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpoch(dataloader,model,criterion, optimizer, scheduler, epoch,classifier_to_use):\n",
    "    targets_all=[]\n",
    "    predicts_all = []\n",
    "    losses = []\n",
    "\n",
    "    model.train()\n",
    "    t = tqdm(dataloader)\n",
    "\n",
    "    for batch_no,(x, y,img_ids) in enumerate(t):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "    \n",
    "        x = model(x)\n",
    "        if(classifier_to_use==\"rgb\"):\n",
    "            color_feats = model.extractColorFeatures(img_ids,feat=\"rgb_feats\")\n",
    "            x = model.fuseFeatures(x,color_feats)\n",
    "            outputs = model.rgbClassifier(x)\n",
    "            loss = criterion(outputs,y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            targets_all.extend(np.argmax(y.cpu().numpy(), axis=1))\n",
    "            predicts_all.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "\n",
    "        elif(classifier_to_use==\"hsv\"):\n",
    "            color_feats = model.extractColorFeatures(img_ids,feat=\"hsv_feats\")\n",
    "            x = model.fuseFeatures(x,color_feats)\n",
    "            outputs = model.hsvClassifier(x)\n",
    "            loss = criterion(outputs,y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            targets_all.extend(np.argmax(y.cpu().numpy(), axis=1))\n",
    "            predicts_all.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "\n",
    "        elif(classifier_to_use==\"hist\"):\n",
    "            color_feats = model.extractColorFeatures(img_ids,feat=\"hist_feats\")\n",
    "            x = model.fuseFeatures(x,color_feats)\n",
    "            outputs = model.histClassifier(x)\n",
    "            loss = criterion(outputs,y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            targets_all.extend(np.argmax(y.cpu().numpy(), axis=1))\n",
    "            predicts_all.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "\n",
    "        else:\n",
    "            # classifier_to_use==\"embedding\"\n",
    "            outputs = model.classifyWithEmbedding(x)\n",
    "            loss = criterion(outputs,y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            targets_all.extend(np.argmax(y.cpu().numpy(), axis=1))\n",
    "            predicts_all.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "        \n",
    "\n",
    "\n",
    "        score = np.mean(targets_all == np.argmax(predicts_all, axis=1))\n",
    "        desc = f\"Training epoch {epoch}/{20} - batch loss:{loss:0.4f}, accuracy: {score:0.4f}\"\n",
    "        t.set_description(desc)\n",
    "        \n",
    "    return np.mean(losses), score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(np.array([[2,3,4,3]])==np.array([[2,3,4,5]]),axis=1)\n",
    "# torch.sigmoid(torch.tensor([2,3,4]))\n",
    "# np.array([[1,2]\n",
    "#           ,[5,3]]).argmax(1)\n",
    "# df.hotel_id[0]\n",
    "# unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "                        optimizer,\n",
    "                        max_lr=1e-3,\n",
    "                        epochs=args.epoch,\n",
    "                        steps_per_epoch=len(train_dataloader),\n",
    "                        div_factor=10,\n",
    "                        final_div_factor=1,\n",
    "                        pct_start=0.1,\n",
    "                        anneal_strategy=\"cos\",\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE =224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_top_1, acc_top_5 = [],[]\n",
    "train_loss, train_score = [],[]\n",
    "prev_valid_acc = 0\n",
    "model_name = f\"rgb_with_embedding-model-{IMG_SIZE}x{IMG_SIZE}\"\n",
    "counter = 0 \n",
    "for epoch in trange(1, 20+1):\n",
    "    training_loss, training_score = trainEpoch(train_dataloader,model, criterion, optimizer, scheduler, epoch,classifier_to_use=\"rgb\")\n",
    "    train_loss.append(training_loss)\n",
    "    train_score.append(training_score)\n",
    "    print(f\"train loss : {train_loss} | train_acc : {train_score}\")\n",
    "    val_acc_top_1, val_acc_top_5 = test_classification(validation_dataloader, model,colorFeat=\"rgb_feats\")\n",
    "    acc_top_1.append(val_acc_top_1)\n",
    "    acc_top_5.append(val_acc_top_5)\n",
    "    if prev_valid_acc<val_acc_top_5:\n",
    "        print(\"model saved..!!\")\n",
    "        # torch.save(model.state_dict(), \"best.pt\")\n",
    "        save_checkpoint(model, scheduler, optimizer, epoch, model_name, train_loss, train_score)\n",
    "        prev_valid_acc = val_acc_top_5\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter +=1\n",
    "    if(counter==5):\n",
    "        print(\"early stopping applied, training done\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_metrics_df = pd.DataFrame({\"acc_top_1\":acc_top_1,\"acc_top_5\":acc_top_5,\"train_loss\":train_loss,\"train_score\":train_score})\n",
    "rgb_metrics_df.to_csv(\"/Users/yhemmy/Documents/code/hotel-id-experiments/artefacts/rgb_metrics_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_classification(validation_dataloader, model,colorFeat=\"rgb_feats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_top_1, acc_top_5 = [],[]\n",
    "train_loss, train_score = [],[]\n",
    "prev_valid_acc = 0\n",
    "model_name = f\"embedding-model-{IMG_SIZE}x{IMG_SIZE}\"\n",
    "counter = 0 \n",
    "for epoch in trange(1, 20+1):\n",
    "    training_loss, training_score = trainEpoch(train_dataloader,model, criterion, optimizer, scheduler, epoch,classifier_to_use=\"embedding\")\n",
    "    train_loss.append(training_loss)\n",
    "    train_score.append(training_score)\n",
    "    print(f\"train loss : {train_loss} | train_acc : {train_score}\")\n",
    "    val_acc_top_1, val_acc_top_5 = test_classification(validation_dataloader, model)\n",
    "    acc_top_1.append(val_acc_top_1)\n",
    "    acc_top_5.append(val_acc_top_5)\n",
    "    if prev_valid_acc<val_acc_top_5:\n",
    "        print(\"model saved..!!\")\n",
    "        # torch.save(model.state_dict(), \"best.pt\")\n",
    "        save_checkpoint(model, scheduler, optimizer, epoch, model_name, train_loss, train_score)\n",
    "        prev_valid_acc = val_acc_top_5\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter +=1\n",
    "    if(counter==5):\n",
    "        print(\"early stopping applied, training done\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_metrics_df = pd.DataFrame({\"acc_top_1\":acc_top_1,\"acc_top_5\":acc_top_5,\"train_loss\":train_loss,\"train_score\":train_score})\n",
    "embedding_metrics_df.to_csv(\"/Users/yhemmy/Documents/code/hotel-id-experiments/artefacts/embedding_metrics_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in trange(1, 5+1):\n",
    "#     train_loss, train_score = trainEpoch(train_dataloader,model, criterion, optimizer, scheduler, epoch,classifier_to_use=\"embedding\")\n",
    "#     print(f\"train loss : {train_loss} | train_acc : {train_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_top_1, acc_top_5 = [],[]\n",
    "train_loss, train_score = [],[]\n",
    "prev_valid_acc = 0\n",
    "model_name = f\"hist_with_embedding-model-{IMG_SIZE}x{IMG_SIZE}\"\n",
    "counter = 0 \n",
    "for epoch in trange(1, 20+1):\n",
    "    training_loss, training_score = trainEpoch(train_dataloader,model, criterion, optimizer, scheduler, epoch,classifier_to_use=\"hist\")\n",
    "    train_loss.append(training_loss)\n",
    "    train_score.append(training_score)\n",
    "    print(f\"train loss : {train_loss} | train_acc : {train_score}\")\n",
    "    val_acc_top_1, val_acc_top_5 = test_classification(validation_dataloader, model,colorFeat=\"hist_feats\")\n",
    "    acc_top_1.append(val_acc_top_1)\n",
    "    acc_top_5.append(val_acc_top_5)\n",
    "    if prev_valid_acc<val_acc_top_5:\n",
    "        print(\"model saved..!!\")\n",
    "        # torch.save(model.state_dict(), \"best.pt\")\n",
    "        save_checkpoint(model, scheduler, optimizer, epoch, model_name, train_loss, train_score)\n",
    "        prev_valid_acc = val_acc_top_5\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter +=1\n",
    "    if(counter==5):\n",
    "        print(\"early stopping applied, training done\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_metrics_df = pd.DataFrame({\"acc_top_1\":acc_top_1,\"acc_top_5\":acc_top_5,\"train_loss\":train_loss,\"train_score\":train_score})\n",
    "hist_metrics_df.to_csv(\"/Users/yhemmy/Documents/code/hotel-id-experiments/artefacts/hist_metrics_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_top_1, acc_top_5 = [],[]\n",
    "train_loss, train_score = [],[]\n",
    "prev_valid_acc = 0\n",
    "model_name = f\"hsv_with_embedding-model-{IMG_SIZE}x{IMG_SIZE}\"\n",
    "counter = 0 \n",
    "for epoch in trange(1, 20+1):\n",
    "    training_loss, training_score = trainEpoch(train_dataloader,model, criterion, optimizer, scheduler, epoch,classifier_to_use=\"hsv\")\n",
    "    train_loss.append(training_loss)\n",
    "    train_score.append(training_score)\n",
    "    print(f\"train loss : {train_loss} | train_acc : {train_score}\")\n",
    "    val_acc_top_1, val_acc_top_5 = test_classification(validation_dataloader, model,colorFeat=\"hsv_feats\")\n",
    "    acc_top_1.append(val_acc_top_1)\n",
    "    acc_top_5.append(val_acc_top_5)\n",
    "    if prev_valid_acc<val_acc_top_5:\n",
    "        print(\"model saved..!!\")\n",
    "        # torch.save(model.state_dict(), \"best.pt\")\n",
    "        save_checkpoint(model, scheduler, optimizer, epoch, model_name, train_loss, train_score)\n",
    "        prev_valid_acc = val_acc_top_5\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter +=1\n",
    "    if(counter==5):\n",
    "        print(\"early stopping applied, training done\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv_metrics_df = pd.DataFrame({\"acc_top_1\":acc_top_1,\"acc_top_5\":acc_top_5,\"train_loss\":train_loss,\"train_score\":train_score})\n",
    "hsv_metrics_df.to_csv(\"/Users/yhemmy/Documents/code/hotel-id-experiments/artefacts/hsv_metrics_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in trange(1, 5+1):\n",
    "#     train_loss, train_score = trainEpoch(train_dataloader,model, criterion, optimizer, scheduler, epoch,classifier_to_use=\"hist\")\n",
    "#     print(f\"train loss : {train_loss} | train_acc : {train_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import trange, tqdm\n",
    "# for i in trange(10):\n",
    "#     print(\"gbasgbos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 5+1):\n",
    "    train_loss, train_score = trainEpoch(train_dataloader,model, criterion, optimizer, scheduler, epoch,classifier_to_use=\"hsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuse_features =trainWithFuseFeaturesPerEpoch(train_dataloader,model)\n",
    "h_classes =trainWithFuseFeaturesPerEpoch(train_dataloader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(fuse_features.shape)\n",
    "print(h_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in img_id:\n",
    "#     f = df[df.image_id==i][\"rgb_feats\"].values[0]\n",
    "#     f =torch.tensor(f,dtype=torch.float).to(DEVICE)\n",
    "#     fuse = torch.cat((fuse_features[-1],f))\n",
    "#     print(f.shape)\n",
    "#     print(fuse_features[0].shape)\n",
    "#     print(fuse[-30:])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exract Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timm.list_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
